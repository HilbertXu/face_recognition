# 机器智能大作业----人脸识别系统代码报告

​                                                          1611453 徐宇成

​                                                           2019年1月18日



[TOC]

***



## 第一部分 环境配置

### 1. ubuntu 与 python虚拟环境的配置

---



操作系统： Ubuntu 16.04

语言版本： Python 3.6.5 （Virtualenv）

需配置前端框架： Keras， Tensorflow-gpu， Pytorch

需配置Python依赖： Numpy， Scikit-learn， matplotlib， opencv， PIL， h5py

<u>***详细的配置方法以及运行指令见项目目录下的README.md文件*</u>**

### 2. CUDA 与 CUDNN 的配置

---



CUDA和CUDNN用来对训练加速，可以提升10倍左右的训练速度

详细安装教程，可能出现的问题以及错误解决见我之前写过的[wiki栏目](http://openbotics.org/wiki/index.php?title=Caffe%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8EOpenCV%EF%BC%8CCUDA,CUDNN%E7%9A%84%E9%85%8D%E7%BD%AE)

内容包含在ubuntu下安装显卡驱动， CUDA， CUDNN以及源码编译安装Opencv 3.3.1， 源码安装Caffe以及Darknet。

### 在环境配置上有问题请联系：hilbertxu@outlook.com

## 第二部分 人脸识别系统的原理以及使用工具

### 3. 实验目的

---



学习对大规模数据进行预处理的方法，以及如何更高效的装载训练数据

学习深度学习框架（keras， tensorflow， pytorch）的使用，如何搭建神经网络模型， 如何使用训练数据训练模型，如何对模型进行苹果

学习如何使用训练好的模型对样本进行预测

学习如何使用训练好的模型对于摄像头中的视频流进行实时识别

### 4. 系统工作原理

---

#### 4.1 卷积神经网络层级结构

我的识别系统使用了迁移学习能力较强的VGG-16网络，其网络结构如下所示：

​		![](/home/kamerider/Pictures/20180228195222196.jpg)	

整个网络可以分成特征提取器和分类器两部分，特征提取器主要包括卷积层，池化层，卷积层用来提取图像的高维特征，池化层用来对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征。

![](/home/kamerider/Pictures/20180228194950318.jpg)

##### 4.1.1 数据输入层

​	输入数据是一个Batch的图像数据

​	是一个四维张量,尺寸为：[batch_size, image_size, image_size, image_channels]

##### 4.1.2 卷积层

​	卷积层中通过生成多个随机权重的卷积掩模对图像进行卷积操作来实现图像高维特征的提取。卷积层可以形象的理解为给图片加上滤镜，对于每一张图像，n个卷积核就相当于给原始图片上加上了n种滤镜，可以获取到原始图像的nge不同的feature maps。卷积神经网络中，通过多层卷积操作，可以提取出每一类图像的高维特征。

​	卷积层中每个卷积核(神经元)都与前一层中位置接近的区域中的多个神经元相连，区域的大小取决于卷积核的大小，在文献中被称为“感受野”。卷积核在工作时，会有按照给定的移动步长规律地扫过输入特征，在感受野内对输入特征做矩阵元素乘法求和并叠加偏差量。

![](/home/kamerider/Pictures/132T62633_0.png)



​	在VGG-16网络中全部使用3*3卷积核、2*2池化核，不断加深网络结构来提升性能，其优点有：

（1）多个一样的3*3的卷积层堆叠非常有用 

（2）两个3*3的卷积层串联相当于1个5*5的卷积层，即一个像素会跟周围5*5的像素产生关联，感受野大小为5*5。

（3）三个3*3的卷积层串联相当于1个7*7的卷积层，但3个串联的3*3的卷积层有更少的参数量，有更多的非线性变换（3次ReLU激活函数），使得CNN对特征的学习能力更强。 

##### 4.1.3 池化层

​	池化层一般接在卷积层之后，用于对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征。一般我们使用的是MaxPooling，即最大池化：在每一个池化区域中找最大值来代替这个区域的特征。

​						![](/home/kamerider/Pictures/1062917-20161117212026498-272435652.png)	

​	

##### 4.1.4 全连接层

​						![](/home/kamerider/Pictures/webwxgetmsgimg.jpeg)

​	在神经网络中，全连接层的主要作用是分类。当数据是线性可分数据的时候，只需要一层隐藏层就可以实现对数据的分类；而当数据是线性不可分的数据的时候，就需要使用隐藏层来进行分类，可以认为将原始输入数据，在每一层隐含层上做了多个二分类，二分类的个数即为该隐含层的神经元个数。

​						![](/home/kamerider/Pictures/20170713133836450.png)



##### 4.1.5 Dropout层

​	在大规模的神经网络中有这样两个缺点：1. 费时；2. 容易过拟合。对于一个有 N 个节点的神经网络，有了 dropout后，就可以看做是 2^N 个模型的集合了，但此时要训练的参数数目却是不变的，这就缓解了费时的问题。

​	Hinton在2014年发表的论文***Dropout: A Simple Way to Prevent Neural Networks from Overfitting* **中做了这样的类比，无性繁殖可以保留大段的优秀基因，而有性繁殖则将基因随机拆了又拆，破坏了大段基因的联合适应性，但是自然选择中选择了有性繁殖，物竞天择，适者生存，可见有性繁殖的强大。dropout 也能达到同样的效果，它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，消除减弱了神经元节点间的联合适应性，增强了泛化能力。

​					![](/home/kamerider/Pictures/1667471-8e10d8a8e14a2ef4.png)



##### 4.1.6 Batch_Normalization

​	我在Keras版本的训练代码中并没有加上Batch_Normalization层，在Pytorch版本的代码中，对于每一层卷积层都加上了Batch_Normalization层来加速巡礼那。结果十分显著，在同样地数据，同样地网络结构下，Keras大概需要20此左右的迭代，训练精度才能稳定在98%以上，而Pytorch大概需要7次迭代就可以达到98%以上的训练精度，加速效果十分显著。

​	Batch_Normalization的基本思想其实相当直观：因为深层神经网络在做非线性变换前的**激活输入值**（就是那个y=Wx+B，x是输入）**随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近**（对于Sigmoid函数来说，意味着激活输入值Wx+B是大的负值或正值），所以这**导致反向传播时低层神经网络的梯度消失**，这是训练深层神经网络收敛越来越慢的**本质原因**，**而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布**，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是**这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。**

##### 4.1.7 激活层

​	把卷积层输出结果作非线性映射。CNN采用的激活函数常为ReLU（The Rectified Linear Unit）/ 修正线性单元， 特点是收敛快，求梯度简单，但是较为脆弱。其函数图像如下：

​			![](/home/kamerider/Pictures/8573609-8f50af5368e23fa5.png)



#### 4.2 实验计划

##### 4.2.1 Keras & Tensorflow

 1. 先对原始脸集进行数据预处理，对每张图片减去全部训练集图片的RGB均值，然后进行图像归一化操作。

 2. 利用Keras搭建VGG-16卷积神经网络模型

 3. 采取多种划分训练集和验证集的方法：

    ​	**手动划分**： 使用sklearn 中的StratifiedShuffleSplitAPI来划分，可以保证划分出来的训练集和验证集均保持原分布

    ​	**KFold交叉验证**：有两种方式，使用sklearn中的API来手动将数据集进行K折分割， 以及使用keras中的sklearn wrapper进行K折验证

    ​	**网络超参数验证（GridSearch）**： 这个方法的目的在于找到最合适的超参数（训练迭代次数epoch， 批大小batch_size， 优化器...）*<u>**但是该方法现在没有很好的实现，会出现内存溢出的问题，是一个未完善的功能**</u>*

	4. 建立学生学号到训练标签的映射

	5. 生成训练数据进入网络，开始训练模型。

	6. 使用训练好的模型进行预测

##### 4.2.2 Pytorch	

1. 使用Pytorch自带的数据读取API进行数据读取，假设文件储存路径是**/DataBase/student_id/images** ，**torchvision.datasets.ImageFolder（）**可以直接读取DataBse中的所有文件夹，并且生成每个文件夹的名称到训练标签的映射，同时还可以对图像进行水平翻转，剪切，去中心化等预处理操作，可以说是一个非常方便的文件读取工具了
2. 将上一步生成的数据集输入网络进行训练
3. 使用训练好的模型进行预测

### 5. 使用工具

---



#### 5.1 Keras & Tensorflow

​			![](/home/kamerider/Pictures/25c3e14ce65c4ecdb4ea97285e05b188.jpeg)

​	Keras是一个高层神经网络API，由Python编写并且使用Tensorflow和Theano作为后端工作，使用Keras可以很方便的编写神经网络，训练网络和使用模型进行预测。但是Keras是Tensorflow的高层封装，并不能很好的理解Tensorflow的特性和工作方式，以及网络的原理；所以我在完成了Keras部分的系统之后，使用纯Tensorflow重新搭建了我的系统，使用更底层的函数进行了神经网络的搭建和训练工作**（模型的搭建和训练代码在./tensorflow文件夹中，可以进行训练，由于模型的使用部分并没有区别，所以就只编写了tensoflow版本的模型搭建既训练部分）**，在完成了Tensoflow版本的代码编写之后，我对于神经网络中每一个网络层的结构组成以及工作原理有了更深的理解， 也学习到了Tensorflow中计算图的概念。

#### 5.2 Pytorch

![](/home/kamerider/Pictures/640.jpeg)

​	在写Tensorflow部分的代码的时候，我认识到Tensorflow使用的是静态计算图，即我们需要先定义计算图，然后在每次运行的时候使用的都是同一个计算图。这样当输入需要变化的时候，就需要重构整个训练图，而且，在调试的时候也不能逐步调试，虽然一定程度上静态图节省了计算量，运行速度更快，但是仍然存在诸多不便。在了解到逐渐兴起的Pytorch使用的是动态计算图，即每次都会重构一个新的计算图，使用者能够用任何他们喜欢的方式进行debug，非常直观；同时Pytorch与Numpy及Python完全契合，语法和接口上具有很高的一致性，使用起来十分方便。之后对然复习任务繁重，但是我仍然对Pytorch产生了巨大兴趣，决定重新写一分Pytorch框架下的人脸识别系统。

#### 5.3 GPU，CUDA 与 CUDNN

​	同时使用GPU和CPU，加快图形处理速度， GPU由上千个流处理器（core）作为运算器，采用单指令多线程（SIMT）模式。

​	CUDA（Compute Unified Device Architecture）是Nvidia公司推出的一种基于新的并行变成模型和指令集架构的通用计算架构，它能利用Nvidia公司推出的GPU的并行引擎进行计算，在处理图形的任务中，比CPU更加高效

​	CUDNN（The NVIDIA CUDA® Deep Neural Network library），是针对深层神经网络的显卡计算加速库。CUDA可以看作是一个工作台，上面配有很多工具，如锤子、螺丝刀等。cuDNN是基于CUDA的深度学习GPU加速库，有了它才能在GPU上完成深度学习的计算。它就相当于工作的工具。但是CUDA这个工作台买来的时候，并没有送扳手。想要在CUDA上运行深度神经网络，就要安装cuDNN，就像你想要拧个螺帽就要把扳手买回来。这样才能使GPU进行深度神经网络的工作，工作速度相较CPU快很多。

### 第三部分 系统实现步骤以及方法

#### 6. 数据加载以及预处理

---



##### 6.1 数据加载

###### 6.1.1 Keras & Tensorflow

​	Keras版本的代码中我是直接采用了将数据全部读取到内存，然后分割成训练数据和验证数据，在分别进行预处理的方式，比较一般，而Tensorflow版本的代码中我在此基础上使用了Tensorflow自带的数据储存格式以及数据读取API，所以选择Tensorflow版本的代码来进行解释。

​	首先是是**读取数据**以及生成相对应的标签。由于在训练时标签需要使用one-hot code，所以标签的编号必须是从 0 到 class_num-1, 而在所有的类别中学号又不是连续的，因此需要生成一个**学号到标签的映射表**，我首先是找出所有人学号中最小的一个，并将其作为**标准值**，然后将每个人的学号减去这个标准值，得到**相对偏移量**，再对这个偏移量进行从小到大的排序，就生成了每个人**学号相对与标准值的偏移量的顺序表**，在这个表中每个人按照学号从小到大排序。之后每遍历一个以学号命名的文件夹时，只需要将文件夹名对应数字减去之前获得的标准值，得到偏移量之后在顺序表中查找偏移量对应的下标便是这个学号对应的训练标签，生成输出如下：

![](/home/kamerider/Pictures/Screenshot from 2019-01-20 15-51-33.png)

​		代码实现如下：

​		![](/home/kamerider/Pictures/Screenshot from 2019-01-20 15-54-02.png)

​	在完成数据读取之后，为了之后调用方便，我将数据存储在Tensorflow提供的标准数据储存格式**TFRecords文件**中。TFRecords文件中储存的是经Tensorflow序列化之后的图像数据和标签数据，能够很大程度上压缩数据储存空间，并且调用的时候相当于调用裸数据流，只需要用对应的解码方式解码即可。（详细可见Tensorflow文件夹下的load_data.py 以及 train.py文件，此处就不再罗列）

​	在训练时我们就可以直接使用Tensorflow中的接口 `tf.data.TFRecordDataset`直接读取预先生成好的TFReocrds文件，并读取成一个tf.dataset类的对象，再通过类函数对这个数据集中的数据进行解码和预处理。

![](/home/kamerider/Pictures/Screenshot from 2019-01-20 16-08-34.png)

###### 6.1.2 Pytorch

​	pytorch中的数据读取十分方便，默认的数据读取方式可以读取每一个类别的文件夹中的所有图片，并直接生成学号到训练标签（0 - （N-1））的映射（实际效果和我自己手动实现的一致）。同时可以直接生成训练批次（Batch）

​	![figure](/home/kamerider/Pictures/Screenshot from 2019-01-20 16-07-03.png)



##### 6.2 数据预处理

​	VGG-16 网络对**数据预处理**的要求较低，我使用的预处理方法只有去中心化，归一化：即减去图像的RGB均值，并将像素值归一到（-0.5 ， 0.5）的范围内。归一化的操作尤为重要，许多情况下缺少归一化会导致网络训练是loss不收敛。

##### 6.3 数据增强（可选）

​	数据增强主要是用于在数据量较小时，人工通过改变一系列图像属性来生成大量数据。这样可以在原始数据较少时仍然能够使网络有较强的泛化能力以及避免过拟合现象。以下是常用数据增强方法（以Keras为例，详见**/keras/src/train_model.py**）:

![](/home/kamerider/Pictures/Screenshot from 2019-01-20 16-10-29.png)

#### 7. 建立模型

---



###### 7.1 Keras & Tensorflow

​	在Keras和Tensorflow中建立模型的方式比较一致，毕竟Keras还是运行在Tensorflow后端上的。两者都是建立一个顺序模型，然后向模型中添加网络层，除输入层外每一层的输入都是上一层的输出，输入层的输入是每一个批次的图像数据，模型最后会输出分类的结果。Keras代码实现如下（Tensorflow版本的见/tensorflow/src/model.py）：

![](/home/kamerider/Pictures/Screenshot from 2019-01-20 16-19-48.png)

###### 7.2 Pytorch

​	Pytorch中并不是一个完整的网络，他将网络分为了特征提取层（features）和分类层（classifier），个人认为这样很好地体现了网络中各个部分的功能，很有助于理解。特征提取层的输入是原始训练图像数据，输出是提取到的图像特征，而分类层的输入特征提取层提取到的特征，输出是分类结果。以下是代码实现如下：

![](/home/kamerider/Pictures/Screenshot from 2019-01-20 16-27-55.png)



#### 8. 训练并保存模型

---



各个框架下模型的训练部分思路都一致，在此就不一一赘述，只是介绍一下训练的思路。

##### 8.1 计算交叉熵

​	一张图片进入模型后，模型的输出在经过Softmax层进行归一化之后，得到的是一个 N维的数组，N是训练数据的类别数。这个数组中第i个元素的值，就是这张图片对应第i类的可能性（0-1），而这张图片的标签同样是一个N维的数组，标签数组中只有一个元素为1，其余都是0，这个元素的位置就是这张图片对应的类别。

​	交叉熵计算公式如下：

​			       	![](/home/kamerider/Pictures/Screenshot from 2019-01-20 16-47-49.png)

​	代码实现如下（Tensorflow的代码实现最为直观，故以其为例）：

![](/home/kamerider/Pictures/Screenshot from 2019-01-20 16-50-36.png)

##### 8.2 选择优化器

​	每个Batch的图片进入到网络之后我们可以计算得到这个Batch的分类输出的平均Loss，而我们希望的就是网络预测地尽可能准确，反映到Loss上就是Loss尽可能地小。这个时候就需要用到优化器来对网络中的参数（权重， 偏置）进行调整。优化器的最终目标是寻找到全局最优解，即最优的网络参数，在提出了Adam算法之后，很多人会直接选用自适应学习率的Adam优化器，但是事实上根据我的多次实验，在图像多分类问题中，效果较好的优化器应该是:

​	**带动量的随机梯度下降优化器**，即SGD（Stochastic Gradient Descent）+ Momentum。基本策略可以理解为随机梯度下降像是一个盲人下山，不用每走一步计算一次梯度，但是他总能下到山底，只不过过程会显得扭扭曲曲。使用动量（Momentum）的随机梯度下降算法（SGD）主要思想是引入了一个积攒历史梯度信息动量来加速SGD计算过程。

​	**自适应学习率优化算法中的Adam算法**，Adam中动量直接并入了梯度一阶矩（指数加权）的估计。其次，相比与缺少修正因子导致二阶矩估计可能在训练初期具有很高偏置的RMSProp， Adam包括偏置修正，修正从原点初始化的一阶矩（动量项）和（非中心的）二阶矩估计。

在Keras中的实现分别如下：

![](/home/kamerider/Pictures/Screenshot%20from%202019-01-20%2013-49-36.png)

##### 8.3 计算精确度

​	虽然精确度是我们衡量模型好坏很重要的标准，但是在训练过程中并未使用到精确度。精确度的计算也比较简单可直观的表视为：

​					**Acc = 正确识别样本书/样本总数**

##### 8.4 回调函数

​	在Keras版本的代码中我引入了一系列的回调函数，来可视化训练过程，记录训练过程，调整学习率，以及实现EarlyStoping功能。

![](/home/kamerider/Pictures/Screenshot from 2019-01-20 17-12-39.png)



#### 9. 使用训练好的模型进行预测

---



##### 9.1 使用摄像头进行实时识别

​	基本思路是先使用OpenCV中的人脸分类器将视频流每一帧中的人脸截取出来，然后作为输入数据输进网络中进行预测，然后得到预测结果

##### 9.2 対测试图片进行批量预测

​	对图片进行批量预测可以类比训练的过程，只是在测试的时候模型参数固定使用训练好的参数不再变化。

**注意！**

​	*在使用模型进行预测时，需要保证输入图片的测试数据经过与训练数据一样的数据预处理。否则可能出现精度较低的情况*



### 第四部分 试验结果及结果评估

#### 10. 模型训练结果

---



##### 10.1 Keras & Tensorflow训练结果

###### 10.1.1 使用DataShuffleSplit训练

​	在加入EarlyStopping机制以及学习率衰减机制之后模型在60次迭代之后停止了训练，以下是训练结果输出：

​	![](/home/kamerider/Pictures/Screenshot%20from%202019-01-20%2017-07-10.png)

​	以下是通过tensorboard可视化的训练过程中的loss曲线和accuracy曲线：

![](/home/kamerider/Pictures/Screenshot from 2019-01-20 17-51-37.png)

###### 10.1.2 使用KFold交叉验证训练

​	同样加入了EarlyStopping机制以及学习率衰减机制，以下是训练过程中最后一层全连接层（分类层）的参数更新情况：

​	![](/home/kamerider/Pictures/Screenshot from 2019-01-20 17-49-22.png)

​	以下是训练过程中loss和accuracy的变化曲线：

![](/home/kamerider/Pictures/Screenshot from 2019-01-20 17-48-36.png)

##### 10.2 Pytorch

​	Pytorch中由于我使用了Batch_Normalization层来加速训练，只需要10次左右的迭代就能获得很好的训练效果。以下是Pytorch的训练效果以及所使用的网络结构：

![](/home/kamerider/Pictures/Screenshot from 2019-01-20 18-30-48.png)

#### 11. 模型测试结果

---



##### 11.1 Keras & Tensorflow

​	选用效果最好的Kfold交叉验证训练得到的模型进行测试

###### 	11.1.1 图片测试效果

​	以下是测试图片图片和识别结果：

​				![](/home/kamerider/machine_learning/face_recognition/keras/test.png)

![](/home/kamerider/Pictures/Screenshot from 2019-01-20 18-41-04.png)

###### 11.1.2 实时视频流识别效果

可以识别出来我，并且正确显示了学号

![](/home/kamerider/machine_learning/face_recognition/keras/camera_test.png)

​	

##### 11.2 Pytorch 

​	Pytorch中我只写了一个简单的测试图像识别demo，以下是使用Pytorch训练出来的模型进行识别的效果，Pytorch中提供了很方便的显示多张图片的API：

​	测试用图片：![](/home/kamerider/Figure_1.png)

​	识别结果：

![](/home/kamerider/Pictures/Screenshot from 2019-01-20 20-46-59.png)



#### 12. 结果评估

---



​	从上面的测试图像可以看到，Keras和Pytorch训练出来的模型都能够在测试图像上有很好的表现，但是在实时视频流的识别上就有所不足。在使用摄像头进行实时识别的时候，会出现误识别的情况，分析可能是受到了光照条件的影响，以及在使用OpenCV从逐帧图像中截取人脸时截取框大小选择不太合适，应该保证截取出来的人脸应该和训练集一样保留少许背景，而不是全部都是人脸。在将截取范围略微改大之后，识别效果好了很多。

​	以及在制作训练集的时候，重复照片过多，光照条件差别不大，每个人的照片差别不大，所以导致了模型的泛化能力不强，考虑在进一步的实验中，对于128x128的原始图像，不采用直接缩放到64x64的预处理方法。而使用中心随机剪裁的方法，即每次在图像中心区域随机剪裁出64x64的区域作为训练数据。同时在图像数据预处理的时候，加上对图像颜色亮度的调整来模拟不同的光照条件。



### 第五部分 实验反思以及未来计划

---



#### 13.1实验反思

​	这是我第一次从头到尾完全自己编写代码完成基于Tensorflow和Pytorch的深度学习任务，先前仅有使用Keras搭建简易神经网络的经验。在这一次实验中我尝试了多种不同的框架，不同的训练方法，以及不同的参数进行了训练。在这个过程中遇到了许许多多的问题，尤其是在数据的读取与预处理的部分以及训练部分，让我对于这种大规模图像数据的处理上积累了许多的实战经验。最开始我会因为图像标签没有转化成one-hot码而出问题，之后花了一整个晚上研究出了如何建立训练标签到实际学号的映射表，当我发现Pytorch自带的API里面使用的方法和我一样时，也是十分有成就感的。

​	这次大作业让我在深度学习任务上的实战能力大大提升，对于经典机器学习算法的理解更加深刻，俗话说“纸上得来终觉浅，绝知此事要躬行”，对于概念十分复杂的机器学习和深度学习，实战是再好不过的学习途径了。

#### 13.2 未来计划

​	我将会继续完善并维护代码，争取在大三期间完善Pytorch下的功能解决Tensoflow版本代码中仍然存在的问题，以及在现有功能的基础上完善前端，制作出来易于用户使用的UI界面。

​	项目Github地址：[https://github.com/HilbertXu/face_recognition.git](https://github.com/HilbertXu/face_recognition.git	)	

​	若有问题请联系： hilbertxu@outlook.com



### 						参考文献

[1] https://tensorflow.google.cn/api_docs/python/tf

[2] https://keras.io/zh/

[3] https://github.com/machrisaa/tensorflow-vgg

[4] https://github.com/yunjey/pytorch-tutorial

[5] http://www.tensorfly.cn/tfdoc/tutorials/overview.html